{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Numba for GPUs\n",
    "\n",
    "You may use Numba to generate GPU code from high-level Python functions. Both CUDA and ROCm GPUs (NVIDIA and AMD, respectively) are supported, but we are going to focus only on the CUDA capabilities of Numba.\n",
    "\n",
    "There is an almost one-to-one mapping between the Numba high-level Python abstractions and the different CUDA constructs. This practically means that, as a programmer, you need to take care explicitly of the host/device memory management and you need to be aware of the CUDA programming model.\n",
    "\n",
    "This demo will teach you to write your first CUDA kernels using Numba. It will cover the basic CUDA programming principles, but it should be enough to kick start you in GPU programming. More specifically, we will cover the following topics:\n",
    "\n",
    "- Writing a GPU kernel\n",
    "- Moving data to/from the GPU.\n",
    "- Spawning a GPU kernel\n",
    "- Profiling a GPU kernel\n",
    "- Optimizing memory accesses\n",
    "- Making use of the shared memory\n",
    "\n",
    "We will not cover CUDA streams.\n",
    "\n",
    "## Verify that Numba sees the GPU and understands CUDA\n",
    "\n",
    "First thing is to check if Numba can detect the GPU. You can achieve this by running the `numba` executable that comes with Numba's installation:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Ziaeemehr/workshop_hpcpy/blob/main/notebooks/numba-cuda/1.numba-cuda-basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System info:\n",
      "--------------------------------------------------------------------------------\n",
      "__Time Stamp__\n",
      "Report started (local time)                   : 2024-12-05 17:52:33.906651\n",
      "UTC start time                                : 2024-12-05 16:52:33.906657\n",
      "Running time (s)                              : 1.870404\n",
      "\n",
      "__Hardware Information__\n",
      "Machine                                       : x86_64\n",
      "CPU Name                                      : skylake\n",
      "CPU Count                                     : 20\n",
      "Number of accessible CPUs                     : 20\n",
      "List of accessible CPUs cores                 : 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n",
      "CFS Restrictions (CPUs worth of runtime)      : None\n",
      "\n",
      "CPU Features                                  : 64bit adx aes avx avx2 bmi bmi2\n",
      "                                                clflushopt cmov crc32 cx16 cx8\n",
      "                                                f16c fma fsgsbase fxsr invpcid\n",
      "                                                lzcnt mmx movbe pclmul pku popcnt\n",
      "                                                prfchw rdrnd rdseed sahf sgx sse\n",
      "                                                sse2 sse3 sse4.1 sse4.2 ssse3\n",
      "                                                xsave xsavec xsaveopt xsaves\n",
      "\n",
      "Memory Total (MB)                             : 64034\n",
      "Memory Available (MB)                         : 50169\n",
      "\n",
      "__OS Information__\n",
      "Platform Name                                 : Linux-5.15.0-122-generic-x86_64-with-glibc2.31\n",
      "Platform Release                              : 5.15.0-122-generic\n",
      "OS Name                                       : Linux\n",
      "OS Version                                    : #132~20.04.1-Ubuntu SMP Fri Aug 30 15:50:07 UTC 2024\n",
      "OS Specific Version                           : ?\n",
      "Libc Version                                  : glibc 2.31\n",
      "\n",
      "__Python Information__\n",
      "Python Compiler                               : GCC 13.3.0\n",
      "Python Implementation                         : CPython\n",
      "Python Version                                : 3.10.15\n",
      "Python Locale                                 : en_US.UTF-8\n",
      "\n",
      "__Numba Toolchain Versions__\n",
      "Numba Version                                 : 0.60.0\n",
      "llvmlite Version                              : 0.43.0\n",
      "\n",
      "__LLVM Information__\n",
      "LLVM Version                                  : 14.0.6\n",
      "\n",
      "__CUDA Information__\n",
      "CUDA Device Initialized                       : True\n",
      "CUDA Driver Version                           : 12.2\n",
      "CUDA Runtime Version                          : 12.0\n",
      "CUDA NVIDIA Bindings Available                : False\n",
      "CUDA NVIDIA Bindings In Use                   : False\n",
      "CUDA Minor Version Compatibility Available    : False\n",
      "CUDA Minor Version Compatibility Needed       : False\n",
      "CUDA Minor Version Compatibility In Use       : False\n",
      "CUDA Detect Output:\n",
      "Found 1 CUDA devices\n",
      "id 0     b'NVIDIA RTX A5000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-d225fdfc-a3f4-7738-6290-be1de774f031\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA Libraries Test Output:\n",
      "Finding driver from candidates:\n",
      "\tlibcuda.so\n",
      "\tlibcuda.so.1\n",
      "\t/usr/lib/libcuda.so\n",
      "\t/usr/lib/libcuda.so.1\n",
      "\t/usr/lib64/libcuda.so\n",
      "\t/usr/lib64/libcuda.so.1\n",
      "Using loader <class 'ctypes.CDLL'>\n",
      "\tTrying to load driver...\tok\n",
      "\t\tLoaded from libcuda.so\n",
      "\tMapped libcuda.so paths:\n",
      "\t\t/usr/lib/x86_64-linux-gnu/libcuda.so.535.183.01\n",
      "Finding nvvm from System\n",
      "\tLocated at /usr/local/cuda/nvvm/lib64/libnvvm.so.4.0.0\n",
      "\tTrying to open library...\tok\n",
      "Finding nvrtc from System\n",
      "\tLocated at /usr/local/cuda/lib64/libnvrtc.so.12.0.76\n",
      "\tTrying to open library...\tok\n",
      "Finding cudart from System\n",
      "\tLocated at /usr/local/cuda/lib64/libcudart.so.12.0.107\n",
      "\tTrying to open library...\tok\n",
      "Finding cudadevrt from System\n",
      "\tLocated at /usr/local/cuda/lib64/libcudadevrt.a\n",
      "\tChecking library...\tok\n",
      "Finding libdevice from System\n",
      "\tLocated at /usr/local/cuda/nvvm/libdevice/libdevice.10.bc\n",
      "\tChecking library...\tok\n",
      "\n",
      "\n",
      "__NumPy Information__\n",
      "NumPy Version                                 : 1.26.4\n",
      "NumPy Supported SIMD features                 : ('MMX', 'SSE', 'SSE2', 'SSE3', 'SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2')\n",
      "NumPy Supported SIMD dispatch                 : ('SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2', 'AVX512F', 'AVX512CD', 'AVX512_KNL', 'AVX512_KNM', 'AVX512_SKX', 'AVX512_CLX', 'AVX512_CNL', 'AVX512_ICL', 'AVX512_SPR')\n",
      "NumPy Supported SIMD baseline                 : ('SSE', 'SSE2', 'SSE3')\n",
      "NumPy AVX512_SKX support detected             : False\n",
      "\n",
      "__SVML Information__\n",
      "SVML State, config.USING_SVML                 : False\n",
      "SVML Library Loaded                           : False\n",
      "llvmlite Using SVML Patched LLVM              : True\n",
      "SVML Operational                              : False\n",
      "\n",
      "__Threading Layer Information__\n",
      "TBB Threading Layer Available                 : False\n",
      "+--> Disabled due to Unknown import problem.\n",
      "OpenMP Threading Layer Available              : True\n",
      "+-->Vendor: GNU\n",
      "Workqueue Threading Layer Available           : True\n",
      "+-->Workqueue imported successfully.\n",
      "\n",
      "__Numba Environment Variable Information__\n",
      "None found.\n",
      "\n",
      "__Conda Information__\n",
      "Conda Build                                   : 24.5.1\n",
      "Conda Env                                     : 24.9.1\n",
      "Conda Platform                                : linux-64\n",
      "Conda Python Version                          : 3.11.5.final.0\n",
      "Conda Root Writable                           : True\n",
      "\n",
      "__Installed Packages__\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       2_gnu    conda-forge\n",
      "asttokens                 3.0.0              pyhd8ed1ab_1    conda-forge\n",
      "bzip2                     1.0.8                h4bc722e_7    conda-forge\n",
      "ca-certificates           2024.8.30            hbcca054_0    conda-forge\n",
      "comm                      0.2.2              pyhd8ed1ab_0    conda-forge\n",
      "cupy-cuda12x              13.3.0                   pypi_0    pypi\n",
      "debugpy                   1.8.9           py310hf71b8c6_0    conda-forge\n",
      "decorator                 5.1.1              pyhd8ed1ab_1    conda-forge\n",
      "exceptiongroup            1.2.2              pyhd8ed1ab_1    conda-forge\n",
      "executing                 2.1.0              pyhd8ed1ab_0    conda-forge\n",
      "fastrlock                 0.8.2                    pypi_0    pypi\n",
      "importlib-metadata        8.5.0              pyha770c72_1    conda-forge\n",
      "ipykernel                 6.29.5          py310h06a4308_0    anaconda\n",
      "ipython                   8.30.0             pyh707e725_0    conda-forge\n",
      "jedi                      0.19.2             pyhd8ed1ab_1    conda-forge\n",
      "joblib                    1.4.2              pyhd8ed1ab_0    conda-forge\n",
      "jupyter_client            8.6.3              pyhd8ed1ab_0    conda-forge\n",
      "jupyter_core              5.7.2              pyh31011fe_1    conda-forge\n",
      "keyutils                  1.6.1                h166bdaf_0    conda-forge\n",
      "krb5                      1.21.3               h659f571_0    conda-forge\n",
      "ld_impl_linux-64          2.43                 h712a8e2_2    conda-forge\n",
      "libblas                   3.9.0           25_linux64_openblas    conda-forge\n",
      "libcblas                  3.9.0           25_linux64_openblas    conda-forge\n",
      "libedit                   3.1.20191231         he28a2e2_2    conda-forge\n",
      "libffi                    3.4.2                h7f98852_5    conda-forge\n",
      "libgcc                    14.2.0               h77fa898_1    conda-forge\n",
      "libgcc-ng                 14.2.0               h69a702a_1    conda-forge\n",
      "libgfortran               14.2.0               h69a702a_1    conda-forge\n",
      "libgfortran5              14.2.0               hd5240d6_1    conda-forge\n",
      "libgomp                   14.2.0               h77fa898_1    conda-forge\n",
      "liblapack                 3.9.0           25_linux64_openblas    conda-forge\n",
      "libllvm14                 14.0.6               hcd5def8_4    conda-forge\n",
      "liblzma                   5.6.3                hb9d3cd8_0    conda-forge\n",
      "liblzma-devel             5.6.3                hb9d3cd8_0    conda-forge\n",
      "libnsl                    2.0.1                hd590300_0    conda-forge\n",
      "libopenblas               0.3.28          pthreads_h94d23a6_1    conda-forge\n",
      "libsodium                 1.0.20               h4ab18f5_0    conda-forge\n",
      "libsqlite                 3.47.0               hadc24fc_1    conda-forge\n",
      "libstdcxx                 14.2.0               hc0a3c3a_1    conda-forge\n",
      "libstdcxx-ng              14.2.0               h4852527_1    conda-forge\n",
      "libuuid                   2.38.1               h0b41bf4_0    conda-forge\n",
      "libxcrypt                 4.4.36               hd590300_1    conda-forge\n",
      "libzlib                   1.3.1                hb9d3cd8_2    conda-forge\n",
      "line_profiler             4.1.3           py310h3788b33_1    conda-forge\n",
      "llvmlite                  0.43.0          py310h1a6248f_1    conda-forge\n",
      "matplotlib-inline         0.1.7              pyhd8ed1ab_0    conda-forge\n",
      "memory_profiler           0.61.0             pyhd8ed1ab_0    conda-forge\n",
      "ncurses                   6.5                  he02047a_1    conda-forge\n",
      "nest-asyncio              1.6.0              pyhd8ed1ab_1    conda-forge\n",
      "numba                     0.60.0          py310h5dc88bb_0    conda-forge\n",
      "numpy                     1.26.4          py310hb13e2d6_0    conda-forge\n",
      "openssl                   3.4.0                hb9d3cd8_0    conda-forge\n",
      "packaging                 24.2               pyhd8ed1ab_2    conda-forge\n",
      "parso                     0.8.4              pyhd8ed1ab_1    conda-forge\n",
      "pcre2                     10.44                hba22ea6_2    conda-forge\n",
      "pexpect                   4.9.0              pyhd8ed1ab_1    conda-forge\n",
      "pickleshare               0.7.5           pyhd8ed1ab_1004    conda-forge\n",
      "pip                       24.3.1             pyh8b19718_0    conda-forge\n",
      "platformdirs              4.3.6              pyhd8ed1ab_1    conda-forge\n",
      "prompt-toolkit            3.0.48             pyha770c72_1    conda-forge\n",
      "psutil                    6.1.0           py310ha75aee5_0    conda-forge\n",
      "ptyprocess                0.7.0              pyhd8ed1ab_1    conda-forge\n",
      "pure_eval                 0.2.3              pyhd8ed1ab_0    conda-forge\n",
      "pygments                  2.18.0             pyhd8ed1ab_1    conda-forge\n",
      "python                    3.10.15         h4a871b0_2_cpython    conda-forge\n",
      "python-dateutil           2.9.0.post0        pyhff2d567_1    conda-forge\n",
      "python_abi                3.10                    5_cp310    conda-forge\n",
      "pyzmq                     26.2.0          py310h71f11fc_3    conda-forge\n",
      "readline                  8.2                  h8228510_1    conda-forge\n",
      "scikit-learn              1.5.1           py310h1128e8f_0    anaconda\n",
      "scipy                     1.14.1          py310hfcf56fc_1    conda-forge\n",
      "setuptools                75.6.0             pyhff2d567_1    conda-forge\n",
      "six                       1.17.0             pyhd8ed1ab_0    conda-forge\n",
      "stack_data                0.6.2              pyhd8ed1ab_0    conda-forge\n",
      "swig                      4.3.0                heed6a68_0    conda-forge\n",
      "threadpoolctl             3.5.0              pyhc1e730c_0    conda-forge\n",
      "tk                        8.6.13          noxft_h4845f30_101    conda-forge\n",
      "tornado                   6.4.2           py310ha75aee5_0    conda-forge\n",
      "traitlets                 5.14.3             pyhd8ed1ab_1    conda-forge\n",
      "typing_extensions         4.12.2             pyha770c72_1    conda-forge\n",
      "tzdata                    2024b                hc8b5060_0    conda-forge\n",
      "wcwidth                   0.2.13             pyhd8ed1ab_1    conda-forge\n",
      "wheel                     0.45.1             pyhd8ed1ab_1    conda-forge\n",
      "xz                        5.6.3                hbcc6ac9_0    conda-forge\n",
      "xz-gpl-tools              5.6.3                hbcc6ac9_0    conda-forge\n",
      "xz-tools                  5.6.3                hb9d3cd8_0    conda-forge\n",
      "zeromq                    4.3.5                h3b0a872_7    conda-forge\n",
      "zipp                      3.21.0             pyhd8ed1ab_1    conda-forge\n",
      "\n",
      "No errors reported.\n",
      "\n",
      "\n",
      "No warnings reported.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "If requested, please copy and paste the information between\n",
      "the dashed (----) lines, or from a given specific section as\n",
      "appropriate.\n",
      "\n",
      "=============================================================\n",
      "IMPORTANT: Please ensure that you are happy with sharing the\n",
      "contents of the information present, any information that you\n",
      "wish to keep private you should remove before sharing.\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this command does not work out of the box for your Numba installation, you may have to set the `CUDA_HOME` environment variable to point to your CUDA installation.\n",
    "\n",
    "## Writing the first kernel\n",
    "\n",
    "Here is a vector addition kernel that takes two vectors as input and writes the sum in a third vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple, right? Let's explain each line of this code.\n",
    "\n",
    "The `@cuda.jit` decorator will compile the following function into a CUDA kernel at runtime. We will see the cost of it later on.\n",
    "\n",
    "A CUDA kernel in Numba is a Python function that does *not* return a value. This is in accordance with CUDA, where kernel functions are declared `void`. The arguments of the function can be either Numpy arrays or scalars of a Numba recognized type.\n",
    "\n",
    "CUDA kernels specify the work to be done by a single GPU thread. Due to the massive hardware parallelism available on the GPU, a single GPU thread gets only a tiny portion of work, in this case, the sumation of a single element in the vectors. Since each element is independent from each other, we can safely create as many threads as the elements of the target vectors and spawn them (we will see how later).\n",
    "\n",
    "Threads on the GPU as organized in groups, called CUDA blocks. There are constraints on the maximum number of threads a block can contain. For the P100 GPUs on Daint, this is 1024. This means that you will need multiple blocks to calculate the sum of large vectors. The blocks that comprise a kernel form the *grid*. Threads inside a block are numbered sequentially and the blocks inside the grid are numbered, too. The figure below shows the arrangement of threads for the vector addition example.\n",
    "\n",
    "![Arrangement of threads in CUDA blocks](figs/cuda-blocks.png)\n",
    "\n",
    "In order to obtain the i-th element of the vector given a block size $B$, you would have to calculate the following:\n",
    "\n",
    "\\begin{equation}\n",
    "i = i_{b}B + i_{t}\n",
    "\\end{equation}\n",
    "\n",
    "where $i_{b}$ is the block index and $i_{t}$ is the thread index. CUDA provides this information and Numba makes it available, so that the above statement would be written as follows:\n",
    "\n",
    "```python\n",
    "i = cuda.blockIdx.x*cuda.blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "Blocks and grids can be three dimensional, thus the `.x` attribute in all of these variables (the other dimensions are accessed through the `.y` and `.z` attributes).\n",
    "\n",
    "Since obtaining the absolute position of a thread in a CUDA is quite common operation, Numba offers the convenience function `grid()`, which essentially does automatically the above calculation:\n",
    "\n",
    "```python\n",
    "i = cuda.grid(1)\n",
    "```\n",
    "\n",
    "The argument passed to the `grid()` function is the number of dimensions of the grid.\n",
    "\n",
    "The next three statements in the code simply check that we don't overrun the arrays in case that their dimension is not a multiple of the block size. In this case, some threads of the last block will remain idle:\n",
    "\n",
    "```python\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "```\n",
    "\n",
    "> The threads inside a block are run in batches of 32 at once, called *warps*. All the threads of the warp execute the same instruction. If the program control flow diverges for some of the threads of the warp, e.g., due to an `if` condition, the branches will be executed sequentially by disabling the non participating threads. This condition is called *warp divergence* and may incur a performance penalty.\n",
    "\n",
    "Finally, the `z[i] = x[i] + y[i]` computes the actual sum.\n",
    "\n",
    "\n",
    "## Preparing the data for the GPU\n",
    "\n",
    "All Numba CUDA kernels operate on data residing on the GPU. That means that the `x`, `y` and `z` arrays must be  transferred from the host (CPU) to the device (accelerator) before calling the CUDA kernel.\n",
    "\n",
    "Let's create first two vectors on the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "N = 1024\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute our reference result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to transfer the vectors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `d_x` and `d_y` are Numpy *array-like* objects that have their data mapped in the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x7f4f57d0a2f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `z` array, we don't need to create it on the host and copy it, since it is essentially an output only array. We can simply allocate it directly on the GPU and copy it out when the kernel finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = cuda.device_array_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will simply allocate an array like `x`, i.e., with the same element type, shape and order, on the GPU.\n",
    "\n",
    "Data is ready, now it's time to call the kernel!\n",
    "\n",
    "## Invoking a CUDA kernel\n",
    "\n",
    "When invoking a CUDA kernel you have to specify the block size to use and the corresponding grid size. Picking the right size of block is not always straightforward, but usually values between 64 and 256 are good enough. \n",
    "\n",
    "> The block size has a direct effect on the *occupancy* of each GPU SM, i.e., how much the actual hardware threads of the SM are utilized, and as a result to the occupancy of the whole GPU. Performance-wise, though, it does not have such a big impact. Nvidia provides a nice tool for calculating the occupancy of the GPU, that you can find [here](https://docs.nvidia.com/cuda/cuda-occupancy-calculator/CUDA_Occupancy_Calculator.xls).\n",
    "\n",
    "For our kernel, we will select a block size of 128 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having decided the block size we need to set up the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziaee/anaconda3/envs/hpc/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 8 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the block and the grid could have been two-dimensional (not in this example), in which case you would just define them as tuples.\n",
    "\n",
    "## Copying back the results\n",
    "\n",
    "As mentioned before, kernels operate on GPU data only. We need a way to transfer back to the host the result, which is the `d_z` array. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = d_z.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the result though to make sure that everything has worked fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(z_ref, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "For completeness and easy reference, here is the whole vector addition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziaee/anaconda3/envs/hpc/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 8 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "# Set up the host vectors\n",
    "N = 1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> 1. Make the array sufficiently large and time the Numpy version of the sum `x + y`.\n",
    "> 2. Now time the call to the CUDA kernel with `%timeit -n1 -r1`. What do you see?\n",
    "> 3. Try timing the CUDA kernels with `%timeit -n1 -r2`. What is happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
