{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164e8996",
   "metadata": {},
   "source": [
    "# JAX Advanced Concepts: PyTree, LAX, XLA, and Scan\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ziaeemehr/workshop_hpcpy/blob/main/notebooks/jax/advanced_concepts.ipynb)\n",
    "\n",
    "This notebook covers advanced JAX concepts that are essential for high-performance computing:\n",
    "- **PyTree**: Working with nested data structures\n",
    "- **LAX**: Low-level operations for performance\n",
    "- **XLA**: Understanding JAX's compilation backend\n",
    "- **Scan**: Efficient loops and sequential operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "from jax import lax\n",
    "from jax.tree_util import tree_map, tree_flatten, tree_unflatten, tree_structure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# Setup for Google Colab or local environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Clone repository if on Colab and not already cloned\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists('/content/workshop_hpcpy'):\n",
    "        print(\"Cloning workshop_hpcpy repository...\")\n",
    "        os.system('git clone https://github.com/Ziaeemehr/workshop_hpcpy.git /content/workshop_hpcpy')\n",
    "    \n",
    "    # Change to notebook directory\n",
    "    os.chdir('/content/workshop_hpcpy/notebooks/jax')\n",
    "    print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4030c",
   "metadata": {},
   "source": [
    "# Part 1: PyTree - Working with Nested Structures\n",
    "\n",
    "PyTrees are a core abstraction in JAX for handling nested containers of arrays. They enable JAX transformations to work seamlessly with complex data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21731c",
   "metadata": {},
   "source": [
    "## 1.1 What is a PyTree?\n",
    "\n",
    "A PyTree is any nested structure made of:\n",
    "- Lists\n",
    "- Tuples\n",
    "- Dictionaries\n",
    "- Named tuples\n",
    "- Custom classes (registered as PyTree nodes)\n",
    "\n",
    "Leaves are typically arrays or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of PyTrees\n",
    "pytree1 = [1, 2, 3]  # List\n",
    "pytree2 = (jnp.array([1.0, 2.0]), jnp.array([3.0, 4.0]))  # Tuple of arrays\n",
    "pytree3 = {'a': jnp.array([1.0]), 'b': jnp.array([2.0])}  # Dictionary\n",
    "pytree4 = {'weights': jnp.ones((3, 3)), 'bias': jnp.zeros(3)}  # Neural network parameters\n",
    "pytree5 = [{'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}, \n",
    "           {'W': jnp.ones((2, 1)), 'b': jnp.zeros(1)}]  # Multiple layers\n",
    "\n",
    "print(\"PyTree examples:\")\n",
    "print(f\"List: {pytree1}\")\n",
    "print(f\"Tuple: {pytree2}\")\n",
    "print(f\"Dictionary: {pytree3}\")\n",
    "print(f\"Nested: {pytree4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b3255",
   "metadata": {},
   "source": [
    "## 1.2 Tree Operations\n",
    "\n",
    "JAX provides utilities to manipulate PyTrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_flatten: Convert PyTree to flat list and structure\n",
    "params = {'weights': jnp.array([[1.0, 2.0], [3.0, 4.0]]), \n",
    "          'bias': jnp.array([0.5, 1.5])}\n",
    "\n",
    "leaves, treedef = tree_flatten(params)\n",
    "print(\"Original PyTree:\")\n",
    "print(params)\n",
    "print(\"\\nFlattened leaves:\")\n",
    "for i, leaf in enumerate(leaves):\n",
    "    print(f\"Leaf {i}: {leaf}\")\n",
    "print(f\"\\nTree structure: {treedef}\")\n",
    "\n",
    "# tree_unflatten: Reconstruct PyTree from leaves and structure\n",
    "reconstructed = tree_unflatten(treedef, leaves)\n",
    "print(\"\\nReconstructed PyTree:\")\n",
    "print(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_map: Apply function to all leaves\n",
    "params = {'weights': jnp.array([[1.0, 2.0], [3.0, 4.0]]), \n",
    "          'bias': jnp.array([0.5, 1.5])}\n",
    "\n",
    "# Double all parameters\n",
    "doubled = tree_map(lambda x: 2 * x, params)\n",
    "print(\"Original:\")\n",
    "print(params)\n",
    "print(\"\\nDoubled:\")\n",
    "print(doubled)\n",
    "\n",
    "# Add two PyTrees element-wise\n",
    "params2 = {'weights': jnp.ones((2, 2)), 'bias': jnp.ones(2)}\n",
    "sum_params = tree_map(lambda x, y: x + y, params, params2)\n",
    "print(\"\\nSum of two PyTrees:\")\n",
    "print(sum_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49360ff",
   "metadata": {},
   "source": [
    "## 1.3 PyTrees with JAX Transformations\n",
    "\n",
    "PyTrees work seamlessly with `grad`, `jit`, `vmap`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gradient of a function with PyTree parameters\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Simple linear model loss.\"\"\"\n",
    "    pred = jnp.dot(x, params['weights']) + params['bias']\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "# Initialize parameters\n",
    "params = {\n",
    "    'weights': jnp.array([[0.1, 0.2], [0.3, 0.4]]),\n",
    "    'bias': jnp.array([0.0, 0.0])\n",
    "}\n",
    "\n",
    "# Sample data\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "y = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Compute gradients (returns PyTree with same structure)\n",
    "grads = grad(loss_fn)(params, x, y)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "print(params)\n",
    "print(\"\\nGradients (same structure):\")\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90648fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gradient descent with PyTree parameters\n",
    "@jit\n",
    "def update(params, x, y, learning_rate):\n",
    "    \"\"\"Perform one gradient descent step.\"\"\"\n",
    "    grads = grad(loss_fn)(params, x, y)\n",
    "    # Use tree_map to update all parameters\n",
    "    return tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "# Training loop\n",
    "params = {\n",
    "    'weights': jnp.array([[0.1, 0.2], [0.3, 0.4]]),\n",
    "    'bias': jnp.array([0.0, 0.0])\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    loss = loss_fn(params, x, y)\n",
    "    params = update(params, x, y, 0.1)\n",
    "    print(f\"Step {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f190c9",
   "metadata": {},
   "source": [
    "## 1.4 Custom PyTree Nodes\n",
    "\n",
    "You can register custom classes as PyTree nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4de2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class MLPParams(NamedTuple):\n",
    "    \"\"\"Parameters for a simple MLP.\"\"\"\n",
    "    w1: jnp.ndarray\n",
    "    b1: jnp.ndarray\n",
    "    w2: jnp.ndarray\n",
    "    b2: jnp.ndarray\n",
    "\n",
    "# NamedTuples are automatically PyTrees in JAX\n",
    "params = MLPParams(\n",
    "    w1=jnp.ones((10, 5)),\n",
    "    b1=jnp.zeros(5),\n",
    "    w2=jnp.ones((5, 2)),\n",
    "    b2=jnp.zeros(2)\n",
    ")\n",
    "\n",
    "# Apply tree_map\n",
    "scaled = tree_map(lambda x: 0.5 * x, params)\n",
    "print(\"Original w1 sum:\", params.w1.sum())\n",
    "print(\"Scaled w1 sum:\", scaled.w1.sum())\n",
    "\n",
    "# Count parameters\n",
    "def count_params(pytree):\n",
    "    return sum(x.size for x in tree_flatten(pytree)[0])\n",
    "\n",
    "print(f\"\\nTotal parameters: {count_params(params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfc2c8",
   "metadata": {},
   "source": [
    "# Part 2: LAX - Low-Level Operations\n",
    "\n",
    "The `jax.lax` module provides low-level operations that are more primitive than `jax.numpy`. These are closer to XLA operations and can be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb58ae",
   "metadata": {},
   "source": [
    "## 2.1 Control Flow with LAX\n",
    "\n",
    "LAX provides functional control flow operations that work with JAX transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.cond: Functional if-else\n",
    "def f_true(x):\n",
    "    return x + 1\n",
    "\n",
    "def f_false(x):\n",
    "    return x - 1\n",
    "\n",
    "x = 5.0\n",
    "result_true = lax.cond(True, f_true, f_false, x)\n",
    "result_false = lax.cond(False, f_true, f_false, x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"cond(True, ...): {result_true}\")\n",
    "print(f\"cond(False, ...): {result_false}\")\n",
    "\n",
    "# More practical example: ReLU with lax.cond\n",
    "@jit\n",
    "def relu_cond(x):\n",
    "    return lax.cond(x > 0, lambda x: x, lambda x: 0.0, x)\n",
    "\n",
    "# Note: For ReLU, jnp.maximum is more efficient\n",
    "print(f\"\\nReLU(-2.0) = {relu_cond(-2.0)}\")\n",
    "print(f\"ReLU(3.0) = {relu_cond(3.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc38856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.switch: Multi-way branch (like switch/case)\n",
    "def option_0(x):\n",
    "    return x ** 2\n",
    "\n",
    "def option_1(x):\n",
    "    return x ** 3\n",
    "\n",
    "def option_2(x):\n",
    "    return jnp.sqrt(x)\n",
    "\n",
    "branches = [option_0, option_1, option_2]\n",
    "x = 4.0\n",
    "\n",
    "for i in range(3):\n",
    "    result = lax.switch(i, branches, x)\n",
    "    print(f\"Branch {i}(x={x}): {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.while_loop: Functional while loop\n",
    "def cond_fun(val):\n",
    "    i, total = val\n",
    "    return i < 10\n",
    "\n",
    "def body_fun(val):\n",
    "    i, total = val\n",
    "    return i + 1, total + i\n",
    "\n",
    "init_val = (0, 0)\n",
    "final_i, final_total = lax.while_loop(cond_fun, body_fun, init_val)\n",
    "\n",
    "print(f\"Sum of 0 to 9: {final_total}\")\n",
    "print(f\"Expected: {sum(range(10))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cab246",
   "metadata": {},
   "source": [
    "## 2.2 LAX Operations\n",
    "\n",
    "LAX provides efficient primitive operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.select: Vectorized conditional (like np.where)\n",
    "x = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "result = lax.select(x > 3, x, -x)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"select(x > 3, x, -x): {result}\")\n",
    "\n",
    "# lax.clamp: Clamp values to range\n",
    "x = jnp.array([-1.0, 0.5, 2.0, 5.0])\n",
    "clamped = lax.clamp(0.0, x, 3.0)  # min, x, max\n",
    "print(f\"\\nx: {x}\")\n",
    "print(f\"clamp(0, x, 3): {clamped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: lax.scan vs Python loop\n",
    "def cumsum_loop(arr):\n",
    "    \"\"\"Cumulative sum using Python loop.\"\"\"\n",
    "    result = jnp.zeros_like(arr)\n",
    "    total = 0\n",
    "    for i in range(len(arr)):\n",
    "        total = total + arr[i]\n",
    "        result = result.at[i].set(total)\n",
    "    return result\n",
    "\n",
    "def cumsum_scan(arr):\n",
    "    \"\"\"Cumulative sum using lax.scan.\"\"\"\n",
    "    def body(carry, x):\n",
    "        new_carry = carry + x\n",
    "        return new_carry, new_carry\n",
    "    \n",
    "    _, result = lax.scan(body, 0.0, arr)\n",
    "    return result\n",
    "\n",
    "arr = jnp.arange(10.0)\n",
    "print(f\"Array: {arr}\")\n",
    "print(f\"Cumsum (loop): {cumsum_loop(arr)}\")\n",
    "print(f\"Cumsum (scan): {cumsum_scan(arr)}\")\n",
    "print(f\"Cumsum (jnp): {jnp.cumsum(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989dc51",
   "metadata": {},
   "source": [
    "# Part 3: XLA - Accelerated Linear Algebra\n",
    "\n",
    "XLA is JAX's compilation backend. Understanding XLA helps you write more efficient code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52133872",
   "metadata": {},
   "source": [
    "## 3.1 Understanding JIT Compilation\n",
    "\n",
    "When you use `@jit`, JAX traces your function and compiles it with XLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "\n",
    "x = jnp.arange(1000000.0)\n",
    "\n",
    "# First call: compilation + execution\n",
    "start = time.time()\n",
    "result = selu_jit(x).block_until_ready()\n",
    "compile_time = time.time() - start\n",
    "\n",
    "# Second call: only execution (cached)\n",
    "start = time.time()\n",
    "result = selu_jit(x).block_until_ready()\n",
    "cached_time = time.time() - start\n",
    "\n",
    "# Non-JIT version\n",
    "start = time.time()\n",
    "result = selu(x).block_until_ready()\n",
    "no_jit_time = time.time() - start\n",
    "\n",
    "print(f\"First JIT call (compile + execute): {compile_time*1000:.2f} ms\")\n",
    "print(f\"Second JIT call (cached): {cached_time*1000:.2f} ms\")\n",
    "print(f\"No JIT: {no_jit_time*1000:.2f} ms\")\n",
    "print(f\"\\nSpeedup (cached): {no_jit_time/cached_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6702aa",
   "metadata": {},
   "source": [
    "## 3.2 Static vs Traced Arguments\n",
    "\n",
    "Some arguments should be static (known at compile time) rather than traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Shape-dependent code\n",
    "@jit\n",
    "def normalize_bad(x):\n",
    "    if x.ndim == 1:\n",
    "        return x / jnp.linalg.norm(x)\n",
    "    else:\n",
    "        return x / jnp.linalg.norm(x, axis=1, keepdims=True)\n",
    "\n",
    "# This will cause recompilation for different shapes\n",
    "x1 = jnp.array([1.0, 2.0, 3.0])\n",
    "x2 = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "try:\n",
    "    result1 = normalize_bad(x1)\n",
    "    result2 = normalize_bad(x2)\n",
    "    print(\"Both calls succeeded but caused recompilation\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b827cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Use static_argnums or separate functions\n",
    "from functools import partial\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def normalize_axis(x, axis):\n",
    "    return x / jnp.linalg.norm(x, axis=axis, keepdims=True)\n",
    "\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "result_axis0 = normalize_axis(x, 0)\n",
    "result_axis1 = normalize_axis(x, 1)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(x)\n",
    "print(\"\\nNormalized (axis=0):\")\n",
    "print(result_axis0)\n",
    "print(\"\\nNormalized (axis=1):\")\n",
    "print(result_axis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8b019",
   "metadata": {},
   "source": [
    "## 3.3 XLA Optimization Tips\n",
    "\n",
    "Understanding what XLA does helps you write faster code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion: XLA fuses operations\n",
    "# Separate operations\n",
    "def separate_ops(x):\n",
    "    a = x + 1\n",
    "    b = a * 2\n",
    "    c = b - 3\n",
    "    return c\n",
    "\n",
    "# Fused (XLA will do this automatically)\n",
    "@jit\n",
    "def fused_ops(x):\n",
    "    return (x + 1) * 2 - 3\n",
    "\n",
    "x = jnp.arange(1000000.0)\n",
    "\n",
    "# Time fused version\n",
    "start = time.time()\n",
    "result = fused_ops(x).block_until_ready()\n",
    "fused_time = time.time() - start\n",
    "\n",
    "print(f\"Fused operations: {fused_time*1000:.2f} ms\")\n",
    "print(\"XLA automatically fuses element-wise operations into a single kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c92d8b",
   "metadata": {},
   "source": [
    "# Part 4: Scan - Efficient Sequential Operations\n",
    "\n",
    "`lax.scan` is crucial for efficient sequential computations in JAX. It's like a functional for-loop that can be JIT-compiled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32213bde",
   "metadata": {},
   "source": [
    "## 4.1 Basic Scan Usage\n",
    "\n",
    "`scan(f, init, xs)` applies `f` sequentially to elements of `xs`, carrying state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a194e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: cumulative sum\n",
    "def body_fn(carry, x):\n",
    "    \"\"\"Body function for scan.\n",
    "    \n",
    "    Args:\n",
    "        carry: State being carried through\n",
    "        x: Current element from input sequence\n",
    "    \n",
    "    Returns:\n",
    "        new_carry: Updated state\n",
    "        output: Value to collect\n",
    "    \"\"\"\n",
    "    new_carry = carry + x\n",
    "    output = new_carry\n",
    "    return new_carry, output\n",
    "\n",
    "# Compute cumulative sum\n",
    "xs = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "init_carry = 0.0\n",
    "final_carry, outputs = lax.scan(body_fn, init_carry, xs)\n",
    "\n",
    "print(f\"Input: {xs}\")\n",
    "print(f\"Cumulative sum: {outputs}\")\n",
    "print(f\"Final carry (total sum): {final_carry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160586d",
   "metadata": {},
   "source": [
    "## 4.2 Scan vs For Loop\n",
    "\n",
    "Why use scan instead of a for loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci using for loop (slow)\n",
    "def fib_loop(n):\n",
    "    result = []\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        result.append(a)\n",
    "        a, b = b, a + b\n",
    "    return jnp.array(result)\n",
    "\n",
    "# Fibonacci using scan (fast, can be JIT-compiled)\n",
    "@jit\n",
    "def fib_scan(n):\n",
    "    def body(carry, _):\n",
    "        a, b = carry\n",
    "        return (b, a + b), a\n",
    "    \n",
    "    _, result = lax.scan(body, (0, 1), None, length=n)\n",
    "    return result\n",
    "\n",
    "n = 20\n",
    "print(f\"First {n} Fibonacci numbers:\")\n",
    "print(f\"Loop: {fib_loop(n)}\")\n",
    "print(f\"Scan: {fib_scan(n)}\")\n",
    "\n",
    "# Benchmark\n",
    "n_large = 1000\n",
    "start = time.time()\n",
    "_ = fib_loop(n_large)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = fib_scan(n_large).block_until_ready()\n",
    "scan_time = time.time() - start\n",
    "\n",
    "print(f\"\\nFor n={n_large}:\")\n",
    "print(f\"Loop: {loop_time*1000:.2f} ms\")\n",
    "print(f\"Scan: {scan_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {loop_time/scan_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bca31",
   "metadata": {},
   "source": [
    "## 4.3 Practical Example: RNN\n",
    "\n",
    "Scan is essential for implementing recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN cell\n",
    "@jit\n",
    "def rnn_cell(carry, x, W_hh, W_xh, b):\n",
    "    \"\"\"Single RNN step.\"\"\"\n",
    "    h = carry\n",
    "    h_new = jnp.tanh(jnp.dot(W_hh, h) + jnp.dot(W_xh, x) + b)\n",
    "    return h_new, h_new\n",
    "\n",
    "# Process sequence with RNN\n",
    "@jit\n",
    "def rnn_forward(params, h0, xs):\n",
    "    \"\"\"Forward pass through RNN.\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary with W_hh, W_xh, b\n",
    "        h0: Initial hidden state\n",
    "        xs: Input sequence (time_steps, input_dim)\n",
    "    \n",
    "    Returns:\n",
    "        outputs: Hidden states at each time step\n",
    "    \"\"\"\n",
    "    def body(carry, x):\n",
    "        return rnn_cell(carry, x, params['W_hh'], params['W_xh'], params['b'])\n",
    "    \n",
    "    _, outputs = lax.scan(body, h0, xs)\n",
    "    return outputs\n",
    "\n",
    "# Initialize RNN parameters\n",
    "key = random.PRNGKey(0)\n",
    "hidden_dim = 4\n",
    "input_dim = 3\n",
    "\n",
    "key, *subkeys = random.split(key, 4)\n",
    "params = {\n",
    "    'W_hh': random.normal(subkeys[0], (hidden_dim, hidden_dim)) * 0.1,\n",
    "    'W_xh': random.normal(subkeys[1], (hidden_dim, input_dim)) * 0.1,\n",
    "    'b': jnp.zeros(hidden_dim)\n",
    "}\n",
    "\n",
    "# Generate input sequence\n",
    "time_steps = 10\n",
    "key, subkey = random.split(key)\n",
    "xs = random.normal(subkey, (time_steps, input_dim))\n",
    "h0 = jnp.zeros(hidden_dim)\n",
    "\n",
    "# Forward pass\n",
    "outputs = rnn_forward(params, h0, xs)\n",
    "\n",
    "print(f\"Input shape: {xs.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"\\nFirst 3 hidden states:\")\n",
    "print(outputs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4d238",
   "metadata": {},
   "source": [
    "## 4.4 Bidirectional Scan\n",
    "\n",
    "You can scan in reverse or both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward scan\n",
    "def cumsum_forward(xs):\n",
    "    def body(carry, x):\n",
    "        new_carry = carry + x\n",
    "        return new_carry, new_carry\n",
    "    _, outputs = lax.scan(body, 0.0, xs)\n",
    "    return outputs\n",
    "\n",
    "# Reverse scan\n",
    "def cumsum_reverse(xs):\n",
    "    def body(carry, x):\n",
    "        new_carry = carry + x\n",
    "        return new_carry, new_carry\n",
    "    _, outputs = lax.scan(body, 0.0, xs, reverse=True)\n",
    "    return outputs\n",
    "\n",
    "xs = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(f\"Input: {xs}\")\n",
    "print(f\"Forward cumsum: {cumsum_forward(xs)}\")\n",
    "print(f\"Reverse cumsum: {cumsum_reverse(xs)}\")\n",
    "\n",
    "# Bidirectional: combine forward and reverse\n",
    "def bidirectional_sum(xs):\n",
    "    forward = cumsum_forward(xs)\n",
    "    reverse = cumsum_reverse(xs)\n",
    "    return forward + reverse\n",
    "\n",
    "print(f\"Bidirectional sum: {bidirectional_sum(xs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cae65d",
   "metadata": {},
   "source": [
    "## 4.5 Scan with Multiple Carries\n",
    "\n",
    "You can carry multiple pieces of state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running mean and variance\n",
    "def running_stats(xs):\n",
    "    \"\"\"Compute running mean and variance.\"\"\"\n",
    "    def body(carry, x):\n",
    "        count, mean, M2 = carry\n",
    "        count = count + 1\n",
    "        delta = x - mean\n",
    "        mean = mean + delta / count\n",
    "        delta2 = x - mean\n",
    "        M2 = M2 + delta * delta2\n",
    "        variance = M2 / count\n",
    "        return (count, mean, M2), (mean, variance)\n",
    "    \n",
    "    init_carry = (0, 0.0, 0.0)\n",
    "    _, (means, variances) = lax.scan(body, init_carry, xs)\n",
    "    return means, variances\n",
    "\n",
    "# Test with random data\n",
    "key = random.PRNGKey(42)\n",
    "xs = random.normal(key, (100,)) * 2.0 + 5.0\n",
    "means, variances = running_stats(xs)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(means)\n",
    "plt.axhline(xs.mean(), color='r', linestyle='--', label='True mean')\n",
    "plt.title('Running Mean')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Mean')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(variances)\n",
    "plt.axhline(xs.var(), color='r', linestyle='--', label='True variance')\n",
    "plt.title('Running Variance')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final mean: {means[-1]:.3f} (true: {xs.mean():.3f})\")\n",
    "print(f\"Final variance: {variances[-1]:.3f} (true: {xs.var():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ecd9d",
   "metadata": {},
   "source": [
    "## 4.6 Advanced: Associative Scan\n",
    "\n",
    "For certain operations, JAX can parallelize scans using associativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60578df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Parallel prefix sum\n",
    "from jax.lax import associative_scan\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "xs = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n",
    "\n",
    "# Regular scan\n",
    "def regular_scan_sum(xs):\n",
    "    def body(carry, x):\n",
    "        return carry + x, carry + x\n",
    "    _, result = lax.scan(body, 0.0, xs)\n",
    "    return result\n",
    "\n",
    "# Associative scan (can be parallelized)\n",
    "def parallel_scan_sum(xs):\n",
    "    return associative_scan(add, xs)\n",
    "\n",
    "result_regular = regular_scan_sum(xs)\n",
    "result_parallel = parallel_scan_sum(xs)\n",
    "\n",
    "print(f\"Input: {xs}\")\n",
    "print(f\"Regular scan: {result_regular}\")\n",
    "print(f\"Parallel scan: {result_parallel}\")\n",
    "print(f\"\\nResults match: {jnp.allclose(result_regular, result_parallel)}\")\n",
    "\n",
    "# Benchmark for large arrays\n",
    "large_xs = jnp.arange(100000.0)\n",
    "\n",
    "start = time.time()\n",
    "_ = regular_scan_sum(large_xs).block_until_ready()\n",
    "regular_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = parallel_scan_sum(large_xs).block_until_ready()\n",
    "parallel_time = time.time() - start\n",
    "\n",
    "print(f\"\\nFor {len(large_xs)} elements:\")\n",
    "print(f\"Regular scan: {regular_time*1000:.2f} ms\")\n",
    "print(f\"Parallel scan: {parallel_time*1000:.2f} ms\")\n",
    "print(f\"Speedup: {regular_time/parallel_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a39ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### PyTree\n",
    "- Abstraction for nested data structures\n",
    "- Works with all JAX transformations\n",
    "- Use `tree_map`, `tree_flatten`, `tree_unflatten`\n",
    "- Essential for managing model parameters\n",
    "\n",
    "### LAX\n",
    "- Low-level operations closer to XLA\n",
    "- Functional control flow: `cond`, `switch`, `while_loop`\n",
    "- More efficient than Python control flow\n",
    "\n",
    "### XLA\n",
    "- JAX's compilation backend\n",
    "- Fuses operations automatically\n",
    "- Use `static_argnums` for shape-dependent code\n",
    "- Understand compilation overhead\n",
    "\n",
    "### Scan\n",
    "- Efficient sequential operations\n",
    "- Essential for RNNs and sequential models\n",
    "- Can be JIT-compiled unlike Python loops\n",
    "- Use `reverse=True` for backward passes\n",
    "- `associative_scan` for parallelizable operations\n",
    "\n",
    "These concepts are fundamental for high-performance computing with JAX!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576705b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
